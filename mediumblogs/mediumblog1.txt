What Is a Vector Database?
And why you should put your embeddings in one
Rupert Thomas
Rupert Thomas

·
Follow

4 min read
·
Jun 28





tldr: Vector databases are optimized for the storage and retrieval of embedding vectors, as generated by AI models. Embedding vectors are mathematical representations of data that capture its essential characteristics and semantic relationships. Unlike traditional databases that excel at finding data based on an exact match, vector databases can efficiently execute similarity searches. They are used in applications such as content search, recommender systems, anomaly and fraud detection.

In this article we will cover:

what is an embedding vector,
how to use vector operations to find similar content,
why a vector database is preferable for storing embedding vectors, and
examples of commonly used vector databases.
Embedding vectors
An embedding vector is a mathematical representation of data that preserves its essential characteristics and semantic relationships. AI models (such as Transformers) generate embedding vectors as intermediate representations, but they are valuable in their own right as a tool for working with data. Images, words, audio and other data types can all be represented in vector form.

For example, in natural language processing (NLP), a sentence or word can be transformed into a vector that encapsulates semantic information such as meaning, context, and syntactic relationships. Similarly, in image processing, an embedding vector can capture visual characteristics, allowing for image similarity comparisons or content-based image retrieval.

Embedding vectors can be visualized as encoding a point (or vector) in multi-dimensional space, whereby each number describes the position on a different axis.


A vector of length 3 encodes a position in a 3-dimensional space. Language models typically encode to more complicated spaces; BERT produces a vector that has length 768. (Image credit: author)
Crucially, the embedding vectors from similar content (such as images, words etc) end up positioned close to each other in the vector space. This property can be exploited to search for similar content without being restricted to exact matches, or to understand…

